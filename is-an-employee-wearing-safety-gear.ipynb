{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport os\n\n# Path to the dataset directory\ndata_dir = '/kaggle/input/is-an-employee-wearing-safety-gear/GearedorNot'\n\n# Ensure the directory exists\nassert os.path.exists(data_dir), \"The dataset directory does not exist.\"\n\n# Create training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',\n    label_mode='int',\n    color_mode='rgb',\n    batch_size=32,\n    image_size=(256, 256),\n    shuffle=True,\n    seed=123,\n    validation_split=0.2,\n    subset='training',\n    interpolation='bilinear',\n    crop_to_aspect_ratio=False\n)\n\n# Create validation dataset\nvalidation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',\n    label_mode='int',\n    color_mode='rgb',\n    batch_size=32,\n    image_size=(256, 256),\n    shuffle=True,\n    seed=123,\n    validation_split=0.2,\n    subset='validation',\n    interpolation='bilinear',\n    crop_to_aspect_ratio=False\n)\n\n# Inspect the dataset\nfor images, labels in train_dataset.take(1):\n    print(f'Image batch shape: {images.shape}')\n    print(f'Label batch shape: {labels.shape}')\n    print(f'Class names: {train_dataset.class_names}')\n\n# Define a simple CNN model\nmodel = models.Sequential([\n    layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(len(train_dataset.class_names), activation='softmax')  # Adjust the number of classes as per your dataset\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=10  # Adjust the number of epochs as needed\n)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(validation_dataset)\nprint(f'Validation Accuracy: {accuracy * 100:.2f}%')\n\n# # Load and preprocess a single image\n# img_path = 'path_to_new_image.jpg'  # Change this to your image path\n# img = tf.keras.preprocessing.image.load_img(img_path, target_size=(256, 256))\n# img_array = tf.keras.preprocessing.image.img_to_array(img)\n# img_array = tf.expand_dims(img_array, 0)  # Create a batch\n\n# # Make a prediction\n# predictions = model.predict(img_array)\n# predicted_class = tf.argmax(predictions[0]).numpy()\n# print(f'Predicted class: {predicted_class}')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-06T11:01:36.622514Z","iopub.execute_input":"2024-08-06T11:01:36.623422Z","iopub.status.idle":"2024-08-06T11:06:51.070253Z","shell.execute_reply.started":"2024-08-06T11:01:36.623378Z","shell.execute_reply":"2024-08-06T11:06:51.068837Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-06 11:01:38.820788: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-06 11:01:38.820939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-06 11:01:38.976035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Found 356 files belonging to 2 classes.\nUsing 285 files for training.\nFound 356 files belonging to 2 classes.\nUsing 71 files for validation.\nImage batch shape: (32, 256, 256, 3)\nLabel batch shape: (32,)\nClass names: ['Not Wearing Safety Gear', 'Wearing Safety Gear']\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - accuracy: 0.5561 - loss: 3.4699 - val_accuracy: 0.6056 - val_loss: 0.6779\nEpoch 2/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.6504 - loss: 0.6324 - val_accuracy: 0.6338 - val_loss: 0.6283\nEpoch 3/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.7775 - loss: 0.5339 - val_accuracy: 0.7042 - val_loss: 0.6053\nEpoch 4/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8382 - loss: 0.3950 - val_accuracy: 0.8028 - val_loss: 0.4421\nEpoch 5/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.9377 - loss: 0.2083 - val_accuracy: 0.7746 - val_loss: 0.4712\nEpoch 6/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.9718 - loss: 0.1022 - val_accuracy: 0.7465 - val_loss: 0.7342\nEpoch 7/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9596 - loss: 0.1245 - val_accuracy: 0.8310 - val_loss: 0.4459\nEpoch 8/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0564 - val_accuracy: 0.8451 - val_loss: 0.4074\nEpoch 9/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.9879 - loss: 0.0298 - val_accuracy: 0.8592 - val_loss: 0.4693\nEpoch 10/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0090 - val_accuracy: 0.8451 - val_loss: 0.4723\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369ms/step - accuracy: 0.8444 - loss: 0.4787\nValidation Accuracy: 84.51%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T11:07:59.207317Z","iopub.execute_input":"2024-08-06T11:07:59.208317Z","iopub.status.idle":"2024-08-06T11:07:59.213573Z","shell.execute_reply.started":"2024-08-06T11:07:59.208267Z","shell.execute_reply":"2024-08-06T11:07:59.212360Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"0.8450704216957092\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}