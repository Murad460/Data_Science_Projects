{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport pandas as pd \nimport numpy as np \nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib.image import imread\nfrom transformers import BertTokenizer,TFBertModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\ntrain_pro_df=pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.sample()\ntrain_df[\"lower_test\"]=train_df[\"text\"].str.lower()\n# =texts.str.lower","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# e=train_df[\"lower_test\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the new column created in last cell\n# df.drop([\"text_lower\"], axis=1, inplace=True)\nimport string\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ntrain_df[\"text_wo_punct\"] = train_df[\"text\"].apply(lambda text: remove_punctuation(text))\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\",\".join(stopwords.words(\"english\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwordsss=set(stopwords.words(\"english\"))\ndef stopword12(text):\n    return \" \".join([word for word in str(text).split() if word is not stopwordsss])\ntrain_df[\"stop_words1\"]=train_df[\"text_wo_punct\"].apply(lambda text: stopword12(text))\ntrain_df.head()\n# r=lambda e : d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntrain_df[\"text_wo_stop\"] = train_df[\"stop_words1\"].apply(lambda text: remove_stopwords(text))\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ncnt = Counter()\nfor text in train_df[\"lower_test\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncollect_c=cnt.most_common()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collect_c[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freqwords=set([w for (w,wn) in cnt.most_common(10)])\ndef common_word(text):\n    return \"  \".join([w for w in str(text).split() if w is not freqwords])\ntrain_df[\"common_word\"]=train_df[\"text_wo_stop\"].apply(lambda text : common_word(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"generated\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text modeling","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vec=CountVectorizer()\ncount_vec.fit(train_df[\"common_word\"])\nprint(count_vec.vocabulary_)\nvect=count_vec.transform(train_df[\"common_word\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vect.shape)\nprint(vect.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tfidfvectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvector_t=TfidfVectorizer()\nvector_t.fit(train_df[\"common_word\"])\n# print(vector_t.vocabulary_)\ntra_vec=vector_t.transform(train_df[\"common_word\"])\nprint(tra_vec.shape)\nprint(tra_vec.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your data\n# df = pd.read_csv('your_dataset.csv')\ntexts = train_df['common_word']\nlabels = train_df['generated']\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Vectorize the text data\nvectorizer = TfidfVectorizer(max_features=5000)\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Train a classifier\nclassifier = LogisticRegression()\nclassifier.fit(X_train_tfidf, y_train)\n\n# Predict on the test set\ny_pred = classifier.predict(X_test_tfidf)\n\n# Evaluate the model\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nsvd = TruncatedSVD(n_components=100)  # Adjust the number of components as needed\ntfidf_reduced = svd.fit_transform(X_train_tfidf)\nprint(tfidf_reduced.shape)\nplt.figure(figsize=(10, 6))\nplt.scatter(tfidf_reduced[:, 0], tfidf_reduced[:, 1], alpha=0.5)\nplt.title('2D Visualization of Reduced TF-IDF Matrix')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nmk=KMeans()\n# mk.fit(X_train_tfidf)\nclust_sk=mk.fit_transform(X_train_tfidf)\nprint(clust_sk.shape)\nplt.figure(figsize=(10, 6))\nplt.scatter(clust_sk[:, 0], clust_sk[:, 1], alpha=0.5)\nplt.title('2D Visualization of Reduced TF-IDF Matrix')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pro_df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntext_data = [\" \".join(map(str, row)) for row in X_train_tfidf]\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_data)\n\n# Convert text to sequences of integers\nsequences = tokenizer.texts_to_sequences(text_data)\n\n# Pad sequences to ensure uniform length\nmax_sequence_length = max(len(seq) for seq in sequences)\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')\n\n# Split sequences into input and output\nX = padded_sequences[:, :-1]\ny = padded_sequences[:, -1]\n\n# Define the neural network architecture\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length - 1),\n    tf.keras.layers.LSTM(100),\n    tf.keras.layers.Dense(len(tokenizer.word_index) + 1, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, y, epochs=100, verbose=1)\n\n# Predict the next word\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"Cars have been around\"\ninput_sequence = tokenizer.texts_to_sequences([input_text])\ninput_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length - 1, padding='pre')\npredicted_index = model.predict(input_sequence, verbose=0)[0]\n\npredicted_index_scalar = predicted_index[0].item()\n# Round the predicted index to the nearest integer\npredicted_index_rounded = int(round(predicted_index_scalar))\n\n# Convert index back to word\npredicted_word = tokenizer.index_word.get(predicted_index_rounded, 'Unknown')\n\nprint(\"Input:\", input_text)\nprint(\"Predicted Next Word:\", predicted_word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer\ntest_on_sequence\npad\nembed\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenize=tokenizer()\ntext=tokenize.fit(X_test)\ntext=test_on_sequence\ntest.paddding\nlayers.Embedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}