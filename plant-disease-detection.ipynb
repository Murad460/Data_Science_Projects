{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8291958,"sourceType":"datasetVersion","datasetId":4925896}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nfrom keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout,Input\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=\"/kaggle/input/rice-plant-diseases-dataset/rice leaf diseases dataset\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rice_image=[]\nfor img in os.listdir(df):\n    for img2 in os.path.join(df,img):\n        path=os.path.join(df,img,img2)\n        for img3 in path:\n            path2=cv2.imread(img3)\n            if path2 is not None:\n                rice_image.append(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rice_img=[]\nfor root,dir,files in os.walk(df):\n    for file in files:\n        path=os.path.join(root,file)\n        path1=cv2.imread(path)\n        if path1 is not None:\n            rice_img.append(path1)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(rice_img))\n# print(rice_img.size())\narray_rice=np.array(rice_img)\nprint(array_rice.shape)\nprint(array_rice.size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=array_rice[:,:,:,:3]\nY=array_rice[:,:,:,3:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1=array_rice\nY1=array_rice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(X.size)\nprint(Y.shape)\nprint(Y.size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,Y_train,X_test,Y_test=train_test_split(X,Y,random_state=42,test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(Y_train.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Sequential([\n    Input(shape=(300,300,3)),\n    Conv2D(32,(3,3),padding=\"same\",activation=\"relu\"),\n    Conv2D(32,(3,3),padding=\"same\",activation=\"relu\"),\n    MaxPooling2D(2,2),\n    Flatten(),\n    Dropout(0.25),\n    Dense(64),\n    Dense(1,activation=\"sigmoid\")\n    \n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X1,Y1,epochs=30,verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Generate a synthetic dataset\nnum_samples = 100\nstate_dim = 4\nnum_actions = 2\n\nstates = np.random.randn(num_samples, state_dim)\nactions = np.random.randint(0, num_actions, size=num_samples)\nrewards = np.random.randn(num_samples)\nnext_states = np.random.randn(num_samples, state_dim)\ndones = np.random.randint(0, 2, size=num_samples).astype(bool)  # Indicates if next_state is terminal\n\n# Hyperparameters\ngamma = 0.99  # Discount factor\nlearning_rate = 0.001\nbatch_size = 64\nepochs = 50\n\n# Create the Q-network\ndef create_q_network(state_dim, num_actions):\n    model = Sequential([\n        Dense(64, activation='relu', input_shape=(state_dim,)),\n        Dense(64, activation='relu'),\n        Dense(num_actions, activation='linear')\n    ])\n    model.compile(optimizer=Adam(learning_rate), loss='mse')\n    return model\n\n# Instantiate the Q-network\nq_network = create_q_network(state_dim, num_actions)\n\n# Training loop\nfor epoch in range(epochs):\n    indices = np.random.permutation(num_samples)\n    for i in range(0, num_samples, batch_size):\n        batch_indices = indices[i:i+batch_size]\n        batch_states = states[batch_indices]\n        batch_actions = actions[batch_indices]\n        batch_rewards = rewards[batch_indices]\n        batch_next_states = next_states[batch_indices]\n        batch_dones = dones[batch_indices]\n\n        # Predict Q-values for current states\n        q_values = q_network.predict(batch_states)\n\n        # Predict Q-values for next states\n        q_next_values = q_network.predict(batch_next_states)\n\n        # Compute target Q-values\n        for j in range(len(batch_states)):\n            if batch_dones[j]:\n                q_values[j, batch_actions[j]] = batch_rewards[j]\n            else:\n                q_values[j, batch_actions[j]] = batch_rewards[j] + gamma * np.max(q_next_values[j])\n\n        # Train the Q-network on the batch\n        q_network.fit(batch_states, q_values, epochs=1, verbose=0)\n\n    print(f\"Epoch {epoch+1}/{epochs} completed\")\n\n# Now you have a trained Q-network\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for num in epoch:\n    for epoch in permutation:\n        batch_indices=indices[i+1]\n        state_indices=state[batch_indices]\n        done_indices=done[done_indices]\n        new_states_indices=new_state[batch_indices]\n        reward_indices=rewar[batch_indices]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nnum_samples=100\nstate=10\naction=4\nbatch_size=32\n# reward=[x if x==X_train % X_test]\nstates=np.random.randint(0,2,num_samples)\nactions=np.random.randn(num_samples,10)\ndones=np.random.randint(0,2,num_samples).astype(bool)\nindices=np.random.permutation(num_samples)\nnext_state=np.random.randn(num_samples,10)\n# batch_size=\nfor i in range(num_samples):\n    \n   \n    batch_indices=indices[i:i+batch_size]\n    batch_dones=dones[batch_indices]\n    batch_reward=states[batch_indices]\n    batch_next_state=next_state[batch_indices]\n    batch_action=actions[batch_indices]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:38:41.492012Z","iopub.execute_input":"2024-05-19T17:38:41.492376Z","iopub.status.idle":"2024-05-19T17:38:41.505614Z","shell.execute_reply.started":"2024-05-19T17:38:41.492348Z","shell.execute_reply":"2024-05-19T17:38:41.504343Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"batch_reward\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:38:42.093670Z","iopub.execute_input":"2024-05-19T17:38:42.094074Z","iopub.status.idle":"2024-05-19T17:38:42.102338Z","shell.execute_reply.started":"2024-05-19T17:38:42.094042Z","shell.execute_reply":"2024-05-19T17:38:42.101263Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"array([1])"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming you have loaded your dataset\n# X: features extracted from brain tumor images\n# y: corresponding labels (0 for no tumor, 1 for tumor)\n# For illustration, we'll create synthetic data\n\nnum_samples = 1000\nstate_dim = 100  # Example feature dimension\nnum_actions = 2  # Binary classification: tumor or no tumor\n\n# Synthetic dataset\nX = np.random.randn(num_samples, state_dim)\ny = np.random.randint(0, num_actions, size=num_samples)\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Generate rewards (1 for correct classification, 0 for incorrect classification)\nrewards = (y_train == y_encoded[:len(y_train)]).astype(int)\n# Define the Q-network:\n# python\n# Copy code\ndef create_q_network(state_dim, num_actions):\n    model = Sequential([\n        Dense(64, activation='relu', input_shape=(state_dim,)),\n        Dense(64, activation='relu'),\n        Dense(num_actions, activation='linear')\n    ])\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return model\n\n# Instantiate the Q-network\nq_network = create_q_network(state_dim, num_actions)\n# Train the Q-network Using the Offline Dataset:\n# python\n# Copy code\ngamma = 0.99  # Discount factor\nbatch_size = 64\nepochs = 50\n\n# Training loop\nfor epoch in range(epochs):\n    indices = np.random.permutation(len(X_train))\n    for i in range(0, len(X_train), batch_size):\n        batch_indices = indices[i:i+batch_size]\n        batch_states = X_train[batch_indices]\n        batch_actions = y_train[batch_indices]\n        batch_rewards = rewards[batch_indices]\n        batch_next_states = X_train[batch_indices]  # In this context, next_states are the same as states\n        batch_dones = np.zeros(batch_size).astype(bool)  # Terminal states are not relevant here\n\n        # Predict Q-values for current states\n        q_values = q_network.predict(batch_states)\n\n        # Predict Q-values for next states\n        q_next_values = q_network.predict(batch_next_states)\n\n        # Compute target Q-values\n        for j in range(len(batch_states)):\n            if batch_dones[j]:\n                q_values[j, batch_actions[j]] = batch_rewards[j]\n            else:\n                q_values[j, batch_actions[j]] = batch_rewards[j] + gamma * np.max(q_next_values[j])\n\n        # Train the Q-network on the batch\n        q_network.fit(batch_states, q_values, epochs=1, verbose=0)\n\n    print(f\"Epoch {epoch+1}/{epochs} completed\")\n\n# Now you have a trained Q-network","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}